# Crowd-Certain Documentation

Welcome to the Crowd-Certain documentation. This documentation provides comprehensive information about the Crowd-Certain library for crowd-sourced label aggregation with uncertainty estimation and confidence scoring.

## Documentation Contents

- [README](README.md) - Overview of the project
- [Installation Guide](INSTALLATION.md) - How to install the library
- [Usage Guide](USAGE.md) - How to use the library
- [Examples](EXAMPLES.md) - Example scripts demonstrating library usage
- [Dashboard](DASHBOARD.md) - Interactive dashboard for visualizing results
- [API Reference](API.md) - Detailed API documentation

## What is Crowd-Certain?

Crowd-Certain is a comprehensive framework for aggregating labels from multiple annotators (crowd workers) while estimating the uncertainty and confidence in the aggregated labels. The library implements various techniques for:

- Calculating worker weights based on their consistency and reliability
- Estimating uncertainty in crowd-sourced labels using multiple techniques
- Generating confidence scores for aggregated labels
- Benchmarking against established crowdsourcing techniques
- Analyzing the relationship between worker strength and label quality

## Quick Links

- [GitHub Repository](https://github.com/artinmajdi/taxonomy)
- [Installation](INSTALLATION.md#installation-methods)
- [Basic Usage](USAGE.md#basic-usage)
- [Advanced Usage](USAGE.md#advanced-usage)
- [Examples](EXAMPLES.md)
- [Dashboard](DASHBOARD.md#running-the-dashboard)
- [API Reference](API.md#api-reference)
